{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/usr/local/lib/python3.8/dist-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html\n",
      "  warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(input_1: handle, weight_1: handle, output_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {output: Buffer(output_2: Pointer(float32), float32, [524288], []),\n",
      "             input: Buffer(input_2: Pointer(float32), float32, [2097152], []),\n",
      "             weight: Buffer(weight_2: Pointer(float32), float32, [65536], [])}\n",
      "  buffer_map = {input_1: input, weight_1: weight, output_1: output} {\n",
      "  attr [IterVar(blockIdx.z: int32, (nullptr), \"ThreadIndex\", \"blockIdx.z\")] \"thread_extent\" = 4;\n",
      "  allocate(output.local: Pointer(local float32), float32, [128]), storage_scope = local;\n",
      "  allocate(input.shared: Pointer(shared float32), float32, [2048]), storage_scope = shared;\n",
      "  allocate(weight.shared: Pointer(shared float32), float32, [4096]), storage_scope = shared;\n",
      "  allocate(input.shared.local: Pointer(local float32), float32, [32]), storage_scope = local;\n",
      "  allocate(weight.shared.local: Pointer(local float32), float32, [64]), storage_scope = local;\n",
      "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 16;\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 16;\n",
      "  attr [IterVar(threadIdx.z: int32, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 2;\n",
      "  attr [IterVar(threadIdx.y: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 2;\n",
      "  attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 1 {\n",
      "    for (f.c.init: int32, 0, 16) {\n",
      "      for (x.c.init: int32, 0, 4) {\n",
      "        let cse_var_1: int32 = ((f.c.init*4) + x.c.init)\n",
      "         {\n",
      "          output.local_1: Buffer(output.local, float32, [4096], [], scope=\"local\")[cse_var_1] = 0f32\n",
      "          output.local_1[(cse_var_1 + 64)] = 0f32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (rc.outer: int32, 0, 4) {\n",
      "      attr [IterVar(threadIdx.z_1: int32, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 2;\n",
      "      attr [IterVar(threadIdx.y_1: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 2;\n",
      "      attr [IterVar(threadIdx.x_1: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 1;\n",
      "      for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner: int32, 0, 512) {\n",
      "        input.shared_1: Buffer(input.shared, float32, [2048], [], scope=\"shared\")[(((threadIdx.z_1*1024) + (threadIdx.y_1*512)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner)] = input[((((((((rc.outer*524288) + (threadIdx.z_1*262144)) + (threadIdx.y_1*131072)) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 16)*4096)) + (blockIdx.y*256)) + (floordiv(floormod(ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 16), 4)*64)) + (blockIdx.x*4)) + floormod(ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner, 4))]\n",
      "      }\n",
      "      attr [IterVar(threadIdx.z_2: int32, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 2;\n",
      "      attr [IterVar(threadIdx.y_2: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 2;\n",
      "      attr [IterVar(threadIdx.x_2: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 1;\n",
      "      for (ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner_1: int32, 0, 1024) {\n",
      "        weight.shared_1: Buffer(weight.shared, float32, [4096], [], scope=\"shared\")[(((threadIdx.z_2*2048) + (threadIdx.y_2*1024)) + ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner_1)] = weight[((((((blockIdx.z*16384) + (threadIdx.z_2*8192)) + (threadIdx.y_2*4096)) + (floordiv(ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner_1, 128)*512)) + (rc.outer*128)) + floormod(ax0.ax1.fused.ax2.fused.ax3.fused.inner.inner.inner_1, 128))]\n",
      "      }\n",
      "      for (rc.inner.outer: int32, 0, 32) {\n",
      "        for (ax1: int32, 0, 4) {\n",
      "          for (ax3: int32, 0, 4) {\n",
      "            let cse_var_2: int32 = ((ax1*4) + ax3)\n",
      "             {\n",
      "              input.shared.local_1: Buffer(input.shared.local, float32, [256], [], scope=\"local\", align=64)[cse_var_2] = input.shared_1[((((rc.inner.outer*64) + (ax1*16)) + (threadIdx.y*4)) + ax3)]\n",
      "              input.shared.local_1[(cse_var_2 + 16)] = input.shared_1[(((((rc.inner.outer*64) + (ax1*16)) + (threadIdx.y*4)) + ax3) + 8)]\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        for (ax0: int32, 0, 16) {\n",
      "          for (ax1_1: int32, 0, 4) {\n",
      "            weight.shared.local_1: Buffer(weight.shared.local, float32, [64], [], scope=\"local\")[((ax0*4) + ax1_1)] = weight.shared_1[((((threadIdx.z*2048) + (ax0*128)) + (rc.inner.outer*4)) + ax1_1)]\n",
      "          }\n",
      "        }\n",
      "        for (rc.inner.inner: int32, 0, 4) {\n",
      "          for (f.c: int32, 0, 16) {\n",
      "            for (x.c: int32, 0, 4) {\n",
      "              let cse_var_7: int32 = (f.c*4)\n",
      "              let cse_var_6: int32 = ((rc.inner.inner*4) + x.c)\n",
      "              let cse_var_5: int32 = (cse_var_7 + x.c)\n",
      "              let cse_var_4: int32 = (cse_var_7 + rc.inner.inner)\n",
      "              let cse_var_3: int32 = (cse_var_5 + 64)\n",
      "               {\n",
      "                output.local_1[cse_var_5] = (output.local_1[cse_var_5] + (input.shared.local_1[cse_var_6]*weight.shared.local_1[cse_var_4]))\n",
      "                output.local_1[cse_var_3] = (output.local_1[cse_var_3] + (input.shared.local_1[(cse_var_6 + 16)]*weight.shared.local_1[cse_var_4]))\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    for (f.inner.inner.inner: int32, 0, 16) {\n",
      "      for (x.inner.inner.inner: int32, 0, 4) {\n",
      "        let cse_var_8: int32 = ((f.inner.inner.inner*4) + x.inner.inner.inner)\n",
      "         {\n",
      "          output[(((((((blockIdx.z*131072) + (threadIdx.z*65536)) + (f.inner.inner.inner*4096)) + (blockIdx.y*256)) + (threadIdx.y*64)) + (blockIdx.x*4)) + x.inner.inner.inner)] = output.local_1[cse_var_8]\n",
      "          output[((((((((blockIdx.z*131072) + (threadIdx.z*65536)) + (f.inner.inner.inner*4096)) + (blockIdx.y*256)) + (threadIdx.y*64)) + (blockIdx.x*4)) + x.inner.inner.inner) + 128)] = output.local_1[(cse_var_8 + 64)]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00214707236"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test conv1x1 dense scheduler\n",
    "# very slow on a dense map\n",
    "conv1x1_dense_scheduler = Conv1x1DenseScheduler(128, 512, 64, \"log/conv1x1_dense.log\")\n",
    "conv1x1_dense_scheduler.n_trial = 100\n",
    "conv1x1_dense_scheduler.rtol = 1e-5\n",
    "conv1x1_dense_scheduler.atol = 1e-3\n",
    "conv1x1_dense_scheduler.autotune(refresh=True)\n",
    "conv1x1_dense_scheduler.build(display=False)\n",
    "conv1x1_dense_scheduler.check(runtype=\"pytorch\")\n",
    "conv1x1_dense_scheduler.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.348793e-05"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test conv1x1 gathered scheduler\n",
    "conv1x1_gathered_scheduler = Conv1x1GatheredScheduler(512, 128, 128, 2, \"log/conv1x1_gathered.log\")\n",
    "conv1x1_gathered_scheduler.n_trial = 50\n",
    "conv1x1_gathered_scheduler.rtol = 1e-5\n",
    "conv1x1_gathered_scheduler.atol = 1e-3\n",
    "conv1x1_gathered_scheduler.autotune(refresh=True)\n",
    "conv1x1_gathered_scheduler.build(display=False)\n",
    "conv1x1_gathered_scheduler.check(runtype=\"pytorch\")\n",
    "conv1x1_gathered_scheduler.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.848044e-05"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test scatter add\n",
    "scatter_add_scheduler = ScatterAddScheduler(64, 512, 128, 2, \"log/scatter_add.log\")\n",
    "scatter_add_scheduler.n_trial = 10\n",
    "scatter_add_scheduler.autotune(refresh=True)\n",
    "scatter_add_scheduler.build(display=False)\n",
    "scatter_add_scheduler.check()\n",
    "scatter_add_scheduler.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005954454"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test conv3x3 gathered scheduler\n",
    "# conv3x3_gathered_scheduler = Conv3x3GatheredScheduler(512, 4, 128, 2, \"log/conv3x3_gathered.log\")\n",
    "# conv3x3_gathered_scheduler.n_trial = 50\n",
    "# conv3x3_gathered_scheduler.rtol = 1e-5\n",
    "# conv3x3_gathered_scheduler.atol = 1e-3\n",
    "# conv3x3_gathered_scheduler.autotune(refresh=True)\n",
    "# conv3x3_gathered_scheduler.build(display=False)\n",
    "# conv3x3_gathered_scheduler.check(runtype=\"pytorch\")\n",
    "\n",
    "# sample = conv3x3_gathered_scheduler._generate_sample()\n",
    "# res_tvm = conv3x3_gathered_scheduler._run_tvm(sample)\n",
    "# res_pytorch = conv3x3_gathered_scheduler._run_pytorch(sample)\n",
    "# for i in range(128):\n",
    "#     for j in range(512):\n",
    "#         if np.sum(res_tvm[i, j, :] - res_pytorch[i, j, :]) > 1e-2:\n",
    "#             print(i, j)\n",
    "\n",
    "# conv3x3_gathered_scheduler.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.398621e-05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test gather scheduler\n",
    "# gather_scheduler = GatherScheduler(64, 512, 128, 2, \"log/gather.log\")\n",
    "# gather_scheduler.n_trial = 10\n",
    "# gather_scheduler.autotune(refresh=True)\n",
    "# gather_scheduler.build(display=False)\n",
    "# gather_scheduler.check()\n",
    "# gather_scheduler.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
